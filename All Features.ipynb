{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aad78304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw dataset loaded: (349, 63)\n",
      "Using ALL 62 raw features (no selection or correlation filtering).\n",
      "Feature matrix final shape: (349, 62)\n",
      "\n",
      "=== Starting Nested 5-Fold CV with Inner Hyperparameter Tuning ===\n",
      "\n",
      "Tuning & evaluating: Random Forest\n",
      "Tuning & evaluating: Extra Trees\n",
      "Tuning & evaluating: XGBoost\n",
      "Tuning & evaluating: Gradient Boosting\n",
      "Tuning & evaluating: HistGradientBoosting\n",
      "Tuning & evaluating: AdaBoost\n",
      "Tuning & evaluating: Logistic Regression (L1)\n",
      "Tuning & evaluating: SVC\n",
      "Tuning & evaluating: Gaussian NB\n",
      "Tuning & evaluating: k-NN\n",
      "\n",
      "Top-3 models selected for ensemble: ['Extra Trees', 'Random Forest', 'HistGradientBoosting']\n",
      "\n",
      "================================================================================\n",
      "FINAL PERFORMANCE TABLE (All Raw Features + Nested CV + Optimal Hyperparameters)\n",
      "================================================================================\n",
      "                   Model  Accuracy  Precision  Recall  F1-Score  ROC-AUC    TPR    FNR    FPR    TNR  TP  FN  FP  TN\n",
      "             Extra Trees    0.8911     0.8942  0.8911    0.8908   0.9288 0.9382 0.0618 0.1579 0.8421 167  11  27 144\n",
      "        Optimus Ensemble    0.8911     0.8951  0.8911    0.8907   0.9305 0.9438 0.0562 0.1637 0.8363 168  10  28 143\n",
      "           Random Forest    0.8854     0.8925  0.8854    0.8847   0.9286 0.9551 0.0449 0.1871 0.8129 170   8  32 139\n",
      "    HistGradientBoosting    0.8854     0.8885  0.8854    0.8850   0.9178 0.9326 0.0674 0.1637 0.8363 166  12  28 143\n",
      "                     SVC    0.8768     0.8802  0.8768    0.8764   0.9124 0.9270 0.0730 0.1754 0.8246 165  13  30 141\n",
      "                 XGBoost    0.8739     0.8755  0.8739    0.8737   0.9184 0.9101 0.0899 0.1637 0.8363 162  16  28 143\n",
      "       Gradient Boosting    0.8739     0.8777  0.8739    0.8735   0.9075 0.9270 0.0730 0.1813 0.8187 165  13  31 140\n",
      "Logistic Regression (L1)    0.8711     0.8729  0.8711    0.8708   0.9183 0.9101 0.0899 0.1696 0.8304 162  16  29 142\n",
      "                AdaBoost    0.8481     0.8483  0.8481    0.8481   0.9013 0.8652 0.1348 0.1696 0.8304 154  24  29 142\n",
      "                    k-NN    0.8395     0.8524  0.8395    0.8377   0.8910 0.9382 0.0618 0.2632 0.7368 167  11  45 126\n",
      "             Gaussian NB    0.7966     0.8168  0.7966    0.7926   0.9074 0.9270 0.0730 0.3392 0.6608 165  13  58 113\n",
      "\n",
      "\n",
      "OPTIMAL HYPERPARAMETERS\n",
      "--------------------------------------------------------------------------------\n",
      "                   Model                                                                Best Parameters\n",
      "           Random Forest          {'criterion': 'entropy', 'max_features': 'sqrt', 'n_estimators': 300}\n",
      "             Extra Trees                                  {'max_features': 'sqrt', 'n_estimators': 300}\n",
      "                 XGBoost  {'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 400, 'subsample': 0.8}\n",
      "       Gradient Boosting                   {'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 500}\n",
      "    HistGradientBoosting                     {'learning_rate': 0.1, 'max_depth': None, 'max_iter': 300}\n",
      "                AdaBoost                                    {'learning_rate': 1.0, 'n_estimators': 400}\n",
      "Logistic Regression (L1)                                                                 {'lr__C': 1.0}\n",
      "                     SVC                                            {'svc__C': 1, 'svc__kernel': 'rbf'}\n",
      "             Gaussian NB                                                               None (no tuning)\n",
      "                    k-NN                             {'knn__n_neighbors': 7, 'knn__weights': 'uniform'}\n",
      "        Optimus Ensemble weights = [1, 1, 1] → ['Extra Trees', 'Random Forest', 'HistGradientBoosting']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate, cross_val_predict, GridSearchCV\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, roc_auc_score, precision_score, recall_score, f1_score\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, ExtraTreesClassifier,\n",
    "    GradientBoostingClassifier, HistGradientBoostingClassifier,\n",
    "    AdaBoostClassifier, VotingClassifier\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# ====================== Reproducibility ======================\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# ====================== 1. Load Raw Data (All Features) ======================\n",
    "data = pd.read_csv(\"ovarian_cleaned_dataset.csv\")\n",
    "print(f\"Raw dataset loaded: {data.shape}\")\n",
    "\n",
    "y = data['TYPE']\n",
    "X = data.drop(columns=['TYPE'])   # All original features — no removal whatsoever\n",
    "\n",
    "print(f\"Using ALL {X.shape[1]} raw features (no selection or correlation filtering).\")\n",
    "print(f\"Feature matrix final shape: {X.shape}\\n\")\n",
    "\n",
    "# ====================== 2. CV Setup ======================\n",
    "cv_outer = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "\n",
    "# ====================== 3. Model + Hyperparameter Grids ======================\n",
    "base_models = {\n",
    "    \"Random Forest\": {\n",
    "        \"model\": RandomForestClassifier(random_state=SEED, n_jobs=-1),\n",
    "        \"params\": {\n",
    "            \"n_estimators\": [300, 500],\n",
    "            \"max_features\": [\"sqrt\", \"log2\"],\n",
    "            \"criterion\": [\"entropy\", \"gini\"]\n",
    "        }\n",
    "    },\n",
    "    \"Extra Trees\": {\n",
    "        \"model\": ExtraTreesClassifier(random_state=SEED, n_jobs=-1),\n",
    "        \"params\": {\n",
    "            \"n_estimators\": [300, 500],\n",
    "            \"max_features\": [\"sqrt\", \"log2\"]\n",
    "        }\n",
    "    },\n",
    "    \"XGBoost\": {\n",
    "        \"model\": XGBClassifier(random_state=SEED, eval_metric='logloss', n_jobs=-1),\n",
    "        \"params\": {\n",
    "            \"n_estimators\": [400, 600],\n",
    "            \"max_depth\": [3, 4, 5],\n",
    "            \"learning_rate\": [0.05, 0.1],\n",
    "            \"subsample\": [0.8, 1.0]\n",
    "        }\n",
    "    },\n",
    "    \"Gradient Boosting\": {\n",
    "        \"model\": GradientBoostingClassifier(random_state=SEED),\n",
    "        \"params\": {\n",
    "            \"n_estimators\": [300, 500],\n",
    "            \"learning_rate\": [0.05, 0.1],\n",
    "            \"max_depth\": [3, 4]\n",
    "        }\n",
    "    },\n",
    "    \"HistGradientBoosting\": {\n",
    "        \"model\": HistGradientBoostingClassifier(random_state=SEED),\n",
    "        \"params\": {\n",
    "            \"max_iter\": [300, 500],\n",
    "            \"learning_rate\": [0.05, 0.1],\n",
    "            \"max_depth\": [None, 10]\n",
    "        }\n",
    "    },\n",
    "    \"AdaBoost\": {\n",
    "        \"model\": AdaBoostClassifier(random_state=SEED),\n",
    "        \"params\": {\n",
    "            \"n_estimators\": [200, 400],\n",
    "            \"learning_rate\": [0.5, 1.0]\n",
    "        }\n",
    "    },\n",
    "    \"Logistic Regression (L1)\": {\n",
    "        \"model\": Pipeline([('scaler', StandardScaler()),\n",
    "                           ('lr', LogisticRegression(penalty='l1', solver='saga', max_iter=5000, random_state=SEED))]),\n",
    "        \"params\": {\n",
    "            \"lr__C\": [0.1, 1.0, 10.0]\n",
    "        }\n",
    "    },\n",
    "    \"SVC\": {\n",
    "        \"model\": Pipeline([('scaler', StandardScaler()),\n",
    "                           ('svc', SVC(probability=True, random_state=SEED))]),\n",
    "        \"params\": {\n",
    "            \"svc__C\": [0.1, 1, 10],\n",
    "            \"svc__kernel\": [\"rbf\", \"linear\"]\n",
    "        }\n",
    "    },\n",
    "    \"Gaussian NB\": {\n",
    "        \"model\": GaussianNB(),\n",
    "        \"params\": {}  # No hyperparameters\n",
    "    },\n",
    "    \"k-NN\": {\n",
    "        \"model\": Pipeline([('scaler', StandardScaler()),\n",
    "                           ('knn', KNeighborsClassifier())]),\n",
    "        \"params\": {\n",
    "            \"knn__n_neighbors\": [3, 5, 7],\n",
    "            \"knn__weights\": [\"uniform\", \"distance\"]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# ====================== 4. Leakage-Free Evaluation with Tuning ======================\n",
    "results = []\n",
    "best_models = {}\n",
    "best_params_list = []\n",
    "\n",
    "print(\"=== Starting Nested 5-Fold CV with Inner Hyperparameter Tuning ===\\n\")\n",
    "\n",
    "for name, config in base_models.items():\n",
    "    print(f\"Tuning & evaluating: {name}\")\n",
    "\n",
    "    inner_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "\n",
    "    if config[\"params\"]:\n",
    "        grid = GridSearchCV(config[\"model\"], config[\"params\"], cv=inner_cv,\n",
    "                            scoring='accuracy', n_jobs=-1)\n",
    "        grid.fit(X, y)\n",
    "        best_model = grid.best_estimator_\n",
    "        best_param_str = str(grid.best_params_)\n",
    "    else:\n",
    "        best_model = config[\"model\"]\n",
    "        best_model.fit(X, y)\n",
    "        best_param_str = \"None (no tuning)\"\n",
    "\n",
    "    best_models[name] = best_model\n",
    "    best_params_list.append({\"Model\": name, \"Best Parameters\": best_param_str})\n",
    "\n",
    "    # Outer CV evaluation (leakage-free)\n",
    "    y_pred = cross_val_predict(best_model, X, y, cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED), n_jobs=-1)\n",
    "    if hasattr(best_model, \"predict_proba\"):\n",
    "        y_proba = cross_val_predict(best_model, X, y, cv=cv_outer, method=\"predict_proba\", n_jobs=-1)[:, 1]\n",
    "    elif hasattr(best_model, \"decision_function\"):\n",
    "        y_proba = cross_val_predict(best_model, X, y, cv=cv_outer, method=\"decision_function\", n_jobs=-1)\n",
    "    else:\n",
    "        y_proba = y_pred\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y, y_pred).ravel()\n",
    "    acc = (tp + tn) / (tp + tn + fp + fn)\n",
    "\n",
    "    results.append({\n",
    "        \"Model\": name,\n",
    "        \"Accuracy\": round(acc, 4),\n",
    "        \"Precision\": round(precision_score(y, y_pred, average='weighted'), 4),\n",
    "        \"Recall\": round(recall_score(y, y_pred, average='weighted'), 4),\n",
    "        \"F1-Score\": round(f1_score(y, y_pred, average='weighted'), 4),\n",
    "        \"ROC-AUC\": round(roc_auc_score(y, y_proba), 4),\n",
    "        \"Error Rate\": round(1 - acc, 4),\n",
    "        \"TP\": int(tp), \"FN\": int(fn), \"FP\": int(fp), \"TN\": int(tn),\n",
    "        \"TPR\": round(tp / (tp + fn), 4) if (tp + fn) > 0 else 0,\n",
    "        \"FNR\": round(fn / (tp + fn), 4) if (tp + fn) > 0 else 0,\n",
    "        \"FPR\": round(fp / (fp + tn), 4) if (fp + tn) > 0 else 0,\n",
    "        \"TNR\": round(tn / (fp + tn), 4) if (fp + tn) > 0 else 0\n",
    "    })\n",
    "\n",
    "# ====================== 5. Optimus Ensemble (Weighted Soft Voting) ======================\n",
    "results_df = pd.DataFrame(results).sort_values(\"Accuracy\", ascending=False).reset_index(drop=True)\n",
    "top3_names = results_df.head(3)[\"Model\"].tolist()\n",
    "print(f\"\\nTop-3 models selected for ensemble: {top3_names}\")\n",
    "\n",
    "estimators = []\n",
    "for name in top3_names:\n",
    "    short = \"\".join([c for c in name if c.isalnum()])[:8].lower()\n",
    "    estimators.append((short, best_models[name]))\n",
    "\n",
    "ensemble = VotingClassifier(estimators=estimators, voting='soft')\n",
    "weight_grid = {'weights': [[1,1,1], [2,1,1], [1,2,1], [1,1,2], [2,2,1], [1,2,2], [2,1,2], [3,1,1], [1,3,1], [1,1,3]]}\n",
    "\n",
    "grid_ens = GridSearchCV(ensemble, weight_grid, cv=cv_outer, scoring='accuracy', n_jobs=-1)\n",
    "grid_ens.fit(X, y)\n",
    "\n",
    "best_ens = grid_ens.best_estimator_\n",
    "best_weights = grid_ens.best_params_['weights']\n",
    "\n",
    "y_pred_ens = cross_val_predict(best_ens, X, y, cv=cv_outer, n_jobs=-1)\n",
    "y_proba_ens = cross_val_predict(best_ens, X, y, cv=cv_outer, method='predict_proba', n_jobs=-1)[:, 1]\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y, y_pred_ens).ravel()\n",
    "\n",
    "ensemble_row = {\n",
    "    \"Model\": \"Optimus Ensemble\",\n",
    "    \"Accuracy\": round(grid_ens.best_score_, 4),\n",
    "    \"Precision\": round(precision_score(y, y_pred_ens, average='weighted'), 4),\n",
    "    \"Recall\": round(recall_score(y, y_pred_ens, average='weighted'), 4),\n",
    "    \"F1-Score\": round(f1_score(y, y_pred_ens, average='weighted'), 4),\n",
    "    \"ROC-AUC\": round(roc_auc_score(y, y_proba_ens), 4),\n",
    "    \"Error Rate\": round(1 - grid_ens.best_score_, 4),\n",
    "    \"TP\": int(tp), \"FN\": int(fn), \"FP\": int(fp), \"TN\": int(tn),\n",
    "    \"TPR\": round(tp / (tp + fn), 4),\n",
    "    \"FNR\": round(fn / (tp + fn), 4),\n",
    "    \"FPR\": round(fp / (fp + tn), 4),\n",
    "    \"TNR\": round(tn / (fp + tn), 4)\n",
    "}\n",
    "\n",
    "results_df = pd.concat([results_df, pd.DataFrame([ensemble_row])], ignore_index=True)\n",
    "results_df = results_df.sort_values(\"Accuracy\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# ====================== Final Output ======================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL PERFORMANCE TABLE (All Raw Features + Nested CV + Optimal Hyperparameters)\")\n",
    "print(\"=\"*80)\n",
    "print(results_df[['Model', 'Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC',\n",
    "                  'TPR', 'FNR', 'FPR', 'TNR', 'TP', 'FN', 'FP', 'TN']].to_string(index=False))\n",
    "\n",
    "print(\"\\n\\nOPTIMAL HYPERPARAMETERS\")\n",
    "print(\"-\"*80)\n",
    "params_df = pd.DataFrame(best_params_list)\n",
    "params_df.loc[len(params_df)] = {\"Model\": \"Optimus Ensemble\", \"Best Parameters\": f\"weights = {best_weights} → {top3_names}\"}\n",
    "print(params_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d7274b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, ExtraTreesClassifier,\n",
    "    GradientBoostingClassifier, HistGradientBoostingClassifier,\n",
    "    AdaBoostClassifier, VotingClassifier\n",
    ")\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Load data using all raw features without any filtering\n",
    "data = pd.read_csv(\"ovarian_cleaned_dataset.csv\")\n",
    "y = data['TYPE']\n",
    "X = data.drop(columns=['TYPE'])\n",
    "\n",
    "cv_outer = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "\n",
    "# Model definitions with lightweight, hyperparameter grids\n",
    "base_models = {\n",
    "    \"Decision Tree\": {\n",
    "        \"model\": DecisionTreeClassifier(random_state=SEED),\n",
    "        \"params\": {\n",
    "            \"max_depth\": [None, 10, 20],\n",
    "            \"min_samples_split\": [2, 5],\n",
    "            \"min_samples_leaf\": [1, 2],\n",
    "            \"criterion\": [\"gini\", \"entropy\"]\n",
    "        }\n",
    "    },\n",
    "    \"Random Forest\": {\n",
    "        \"model\": RandomForestClassifier(random_state=SEED, n_jobs=-1),\n",
    "        \"params\": {\"n_estimators\": [300, 500], \"max_features\": [\"sqrt\", \"log2\"], \"criterion\": [\"gini\", \"entropy\"]}\n",
    "    },\n",
    "    \"Extra Trees\": {\n",
    "        \"model\": ExtraTreesClassifier(random_state=SEED, n_jobs=-1),\n",
    "        \"params\": {\"n_estimators\": [300, 500], \"max_features\": [\"sqrt\", \"log2\"]}\n",
    "    },\n",
    "    \"XGBoost\": {\n",
    "        \"model\": XGBClassifier(random_state=SEED, eval_metric='logloss', n_jobs=-1),\n",
    "        \"params\": {\n",
    "            \"n_estimators\": [400, 600],\n",
    "            \"max_depth\": [3, 4, 5],\n",
    "            \"learning_rate\": [0.05, 0.1],\n",
    "            \"subsample\": [0.8, 1.0]\n",
    "        }\n",
    "    },\n",
    "    \"Gradient Boosting\": {\n",
    "        \"model\": GradientBoostingClassifier(random_state=SEED),\n",
    "        \"params\": {\"n_estimators\": [300, 500], \"learning_rate\": [0.05, 0.1], \"max_depth\": [3, 4]}\n",
    "    },\n",
    "    \"HistGradientBoosting\": {\n",
    "        \"model\": HistGradientBoostingClassifier(random_state=SEED),\n",
    "        \"params\": {\"max_iter\": [300, 500], \"learning_rate\": [0.05, 0.1]}\n",
    "    },\n",
    "    \"AdaBoost\": {\n",
    "        \"model\": AdaBoostClassifier(random_state=SEED),\n",
    "        \"params\": {\"n_estimators\": [200, 400], \"learning_rate\": [0.5, 1.0]}\n",
    "    },\n",
    "    \"Logistic Regression (L1)\": {\n",
    "        \"model\": Pipeline([('scaler', StandardScaler()),\n",
    "                           ('lr', LogisticRegression(penalty='l1', solver='saga', max_iter=5000, random_state=SEED))]),\n",
    "        \"params\": {\"lr__C\": [0.1, 1.0, 10.0]}\n",
    "    },\n",
    "    \"SVC\": {\n",
    "        \"model\": Pipeline([('scaler', StandardScaler()),\n",
    "                           ('svc', SVC(probability=True, random_state=SEED))]),\n",
    "        \"params\": {\"svc__C\": [0.1, 1, 10], \"svc__kernel\": [\"rbf\", \"linear\"]}\n",
    "    },\n",
    "    \"Gaussian NB\": {\"model\": GaussianNB(), \"params\": {}},\n",
    "    \"k-NN\": {\n",
    "        \"model\": Pipeline([('scaler', StandardScaler()), ('knn', KNeighborsClassifier())]),\n",
    "        \"params\": {\"knn__n_neighbors\": [3, 5, 7], \"knn__weights\": [\"uniform\", \"distance\"]}\n",
    "    }\n",
    "}\n",
    "\n",
    "results = []\n",
    "best_models = {}\n",
    "best_params_list = []\n",
    "\n",
    "for name, config in base_models.items():\n",
    "    # Inner CV for hyperparameter tuning (nested CV ensures unbiased performance)\n",
    "    if config[\"params\"]:\n",
    "        grid = GridSearchCV(config[\"model\"], config[\"params\"], cv=5, scoring='accuracy', n_jobs=-1)\n",
    "        grid.fit(X, y)\n",
    "        best_model = grid.best_estimator_\n",
    "        best_param_str = str(grid.best_params_)\n",
    "    else:\n",
    "        best_model = config[\"model\"].fit(X, y)\n",
    "        best_param_str = \"None\"\n",
    "\n",
    "    best_models[name] = best_model\n",
    "    best_params_list.append({\"Model\": name, \"Best Parameters\": best_param_str})\n",
    "\n",
    "    # Out-of-fold predictions (outer CV)\n",
    "    y_pred = cross_val_predict(best_model, X, y, cv=cv_outer, n_jobs=-1)\n",
    "    y_proba = (cross_val_predict(best_model, X, y, cv=cv_outer, method=\"predict_proba\", n_jobs=-1)[:, 1]\n",
    "               if hasattr(best_model, \"predict_proba\") else y_pred)\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y, y_pred).ravel()\n",
    "\n",
    "    results.append({\n",
    "        \"Model\": name,\n",
    "        \"Accuracy\": round((tp + tn) / len(y), 4),\n",
    "        \"Precision\": round(precision_score(y, y_pred, average='weighted'), 4),\n",
    "        \"Recall\": round(recall_score(y, y_pred, average='weighted'), 4),\n",
    "        \"F1-Score\": round(f1_score(y, y_pred, average='weighted'), 4),\n",
    "        \"ROC-AUC\": round(roc_auc_score(y, y_proba), 4),\n",
    "        \"TP\": tp, \"FN\": fn, \"FP\": fp, \"TN\": tn,\n",
    "        \"TPR\": round(tp / (tp + fn), 4),\n",
    "        \"FNR\": round(fn / (tp + fn), 4),\n",
    "        \"FPR\": round(fp / (fp + tn), 4),\n",
    "        \"TNR\": round(tn / (fp + tn), 4)\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values(\"Accuracy\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Optimus Ensemble: weighted soft voting of top-3 individually optimized models\n",
    "top3 = results_df.head(3)[\"Model\"].tolist()\n",
    "estimators = [(name.replace(\" \", \"\").replace(\"(\", \"\").replace(\")\", \"\").lower()[:8], best_models[name]) for name in top3]\n",
    "\n",
    "ensemble = VotingClassifier(estimators=estimators, voting='soft')\n",
    "grid_ens = GridSearchCV(ensemble,\n",
    "                        {'weights': [[1,1,1],[2,1,1],[1,2,1],[1,1,2],[3,1,1],[1,3,1],[1,1,3],[2,2,1],[2,1,2],[1,2,2]]},\n",
    "                        cv=cv_outer, scoring='accuracy', n_jobs=-1)\n",
    "grid_ens.fit(X, y)\n",
    "\n",
    "y_pred_ens = cross_val_predict(grid_ens.best_estimator_, X, y, cv=cv_outer, n_jobs=-1)\n",
    "y_proba_ens = cross_val_predict(grid_ens.best_estimator_, X, y, cv=cv_outer, method='predict_proba', n_jobs=-1)[:, 1]\n",
    "tn, fp, fn, tp = confusion_matrix(y, y_pred_ens).ravel()\n",
    "\n",
    "# Final tables\n",
    "print(\"=\"*100)\n",
    "print(\"BASE MODELS (Nested 5-Fold CV on All Raw Features)\")\n",
    "print(\"=\"*100)\n",
    "print(results_df[['Model','Accuracy','Precision','Recall','F1-Score','ROC-AUC','TPR','FNR','FPR','TNR','TP','FN','FP','TN']].to_string(index=False))\n",
    "\n",
    "print(\"\\n\"+\"=\"*100)\n",
    "print(\"OPTIMUS ENSEMBLE (Weighted Soft Voting of Top-3 Models)\")\n",
    "print(\"=\"*100)\n",
    "ens_df = pd.DataFrame([{\n",
    "    \"Model\": \"Optimus Ensemble\",\n",
    "    \"Accuracy\": round(grid_ens.best_score_, 4),\n",
    "    \"Precision\": round(precision_score(y, y_pred_ens, average='weighted'), 4),\n",
    "    \"Recall\": round(recall_score(y, y_pred_ens, average='weighted'), 4),\n",
    "    \"F1-Score\": round(f1_score(y, y_pred_ens, average='weighted'), 4),\n",
    "    \"ROC-AUC\": round(roc_auc_score(y, y_proba_ens), 4),\n",
    "    \"TP\": tp, \"FN\": fn, \"FP\": fp, \"TN\": tn,\n",
    "    \"TPR\": round(tp/(tp+fn), 4),\n",
    "    \"FNR\": round(fn/(tp+fn), 4),\n",
    "    \"FPR\": round(fp/(fp+tn), 4),\n",
    "    \"TNR\": round(tn/(fp+tn), 4)\n",
    "}])\n",
    "print(ens_df[['Model','Accuracy','Precision','Recall','F1-Score','ROC-AUC','TPR','FNR','FPR','TNR','TP','FN','FP','TN']].to_string(index=False))\n",
    "\n",
    "print(\"\\nOPTIMAL HYPERPARAMETERS\")\n",
    "print(\"-\"*80)\n",
    "params_df = pd.DataFrame(best_params_list)\n",
    "params_df.loc[len(params_df)] = [\"Optimus Ensemble\", f\"weights = {grid_ens.best_params_['weights']} → {top3}\"]\n",
    "print(params_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4740abc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ovarian Cancer Env",
   "language": "python",
   "name": "ovarian_cancer_env"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
